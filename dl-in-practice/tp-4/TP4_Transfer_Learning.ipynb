{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Practical session on Transfer Learning**\n",
        "This Pratical session proposes to study several techniques for improving challenging context, in which few data and resources are available."
      ],
      "metadata": {
        "id": "4jVkOWmgFT1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Authors: GOMEZ, Diego, REGUEIRO ESPIÑO Ramón Daniel, SALAZAR, Israfel\n",
        "\n",
        "email: {diego.gomez, ramon.regueiro_espino, israfel.salazar} @ens-paris-saclay.fr\n"
      ],
      "metadata": {
        "id": "CQ4ICVi0XlPE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLKnIngy_2hg"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "**Context :**\n",
        "\n",
        "Assume we are in a context where few \"gold\" labeled data are available for training, say \n",
        "\n",
        "$$X_{\\text{train}} = \\{(x_n,y_n)\\}_{n\\leq N_{\\text{train}}}$$\n",
        "\n",
        "where $N_{\\text{train}}$ is small. \n",
        "\n",
        "A large test set $X_{\\text{test}}$ as well as a large amount of unlabeled data, $X$, is available. We also assume that we have a limited computational budget (e.g., no GPUs).\n",
        "\n",
        "**Instructions to follow :** \n",
        "\n",
        "For each question, write a commented *Code* or a complete answer as a *Markdown*. When the objective of a question is to report a CNN accuracy, please use the following format to report it, at the end of the question :\n",
        "\n",
        "| Model | Number of  epochs  | Train accuracy | Test accuracy |\n",
        "|------|------|------|------|\n",
        "|   XXX  | XXX | XXX | XXX |\n",
        "\n",
        "If applicable, please add the field corresponding to the  __Accuracy on Full Data__ as well as a link to the __Reference paper__ you used to report those numbers. (You do not need to train a CNN on the full CIFAR10 dataset!)\n",
        "\n",
        "In your final report, please *keep the logs of each training procedure* you used. We will only run this jupyter if we have some doubts on your implementation. \n",
        "\n",
        "The total file sizes should be reasonable (feasible with 2MB only!). You will be asked to hand in the notebook, together with any necessary files required to run it if any.\n",
        "\n",
        "You can use https://colab.research.google.com/ to run your experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmTCQPSh_2hg"
      },
      "source": [
        "## Training set creation\n",
        "__Question 1 (2 points) :__ Propose a dataloader to obtain a training loader that will only use the first 100 samples of the CIFAR-10 training set.\n",
        "\n",
        "Additional information :  \n",
        "\n",
        "*   CIFAR10 dataset : https://en.wikipedia.org/wiki/CIFAR-10\n",
        "*   You can directly use the dataloader framework from Pytorch.\n",
        "*   Alternatively you can modify the file : https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics\n",
        "from google.colab import output\n",
        "output.clear()"
      ],
      "metadata": {
        "id": "reGnmjDt41zB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "UMrA9lphYyO5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uZkC5IxR_2hh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb5725aa-8f2f-4908-9b59-5ac0b16cdc63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "def train_100_loader(train_transform,n_samples=100):\n",
        "  train_set = datasets.CIFAR10(root='./data', train=True,\n",
        "                             download=True, transform=train_transform)\n",
        "  train_100 = torch.utils.data.Subset(train_set, np.arange(n_samples)) \n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_100, batch_size=batch_size,\n",
        "                                           shuffle=True, num_workers=2)\n",
        "\n",
        "  return train_loader\n",
        "\n",
        "# as an alternative, we consider every step to use after the whole train dataset on the accuracy evaluation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "n_samples = 100\n",
        "batch_size = 10\n",
        "\n",
        "train_set = datasets.CIFAR10(root='./data', train=True,\n",
        "                             download=True, transform=train_transform)\n",
        "train_100 = torch.utils.data.Subset(train_set, np.arange(n_samples)) \n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_100, batch_size=batch_size,\n",
        "                                           shuffle=True, num_workers=2)\n",
        "\n",
        "test_set = datasets.CIFAR10(root='./data', train=False,\n",
        "                             download=True, transform=test_transform)\n",
        "\n",
        "full_set = torch.utils.data.ConcatDataset([train_set, test_set])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUno1nmu_2hh"
      },
      "source": [
        "* This is our dataset $X_{\\text{train}}$, it will be used until the end of this project. \n",
        "\n",
        "* The remaining samples correspond to $X$. \n",
        "\n",
        "* The testing set $X_{\\text{test}}$ corresponds to the whole testing set of CIFAR-10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr0d4o5L_2hi"
      },
      "source": [
        "## Testing procedure\n",
        "__Question 2 (1.5 points):__ Explain why the evaluation of the training procedure is difficult. Propose several solutions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppiTrnpd_2hi"
      },
      "source": [
        "The problem we have is that the training set is small, only 100 observations. Hence, we might easily overfit on the training data. Specially, taking into account that we have 10 different classes, the class distribution on the train set might be very different for the one on the whole dataset. Moreover, if we want to use a validation set we will increase the impact of the small size of the dataset. Hence, to diminish these problems we propose the use of the following tools:\n",
        "\n",
        "- Use Transfer Learning.\n",
        "- To perform data augmentation techniques.\n",
        "- To consider semi-supervised or self-supervised learning.\n",
        "- Perform a $K$-fold to validate the model.\n",
        "\n",
        "\n",
        "We remark that the collected tools can be complementary between themselves, for instance, using a pre-trained model and performing $K$-fold instead of a validation set. In addition, there exist other possible solutions considering Deep Learning approaches like Adversarial training or GAN augmentation. However, these solutions require a extremely high computational cost which is not valid taking into account the fact that we have a limited computational budget."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEaIwILB_2hi"
      },
      "source": [
        "# The Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-PQZ2Vl_2hi"
      },
      "source": [
        "In this section, the goal is to train a CNN on $X_{\\text{train}}$ and compare its performance with reported numbers from the litterature. You will have to re-use and/or design a standard classification pipeline. You should optimize your pipeline to obtain the best performances (image size, data augmentation by flip, ...).\n",
        "\n",
        "The key ingredients for training a CNN are the batch size, as well as the learning rate scheduler (i.e. how to decrease the learning rate as a function of the number of epochs). A possible scheduler is to start the learning rate at 0.1 and decreasing it every 30 epochs by 10. In case of divergence, reduce the learning rate. A potential batch size could be 10, yet this can be cross-validated.\n",
        "\n",
        "You can get some baselines accuracies in this paper (obviously, it is a different context for those researchers who had access to GPUs!) : http://openaccess.thecvf.com/content_cvpr_2018/papers/Keshari_Learning_Structure_and_CVPR_2018_paper.pdf. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARHWPXrY_2hi"
      },
      "source": [
        "## ResNet architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voMbGoNw_2hj"
      },
      "source": [
        "__Question 3 (4 points) :__ Write a classification pipeline for $X_{\\text{train}}$, train from scratch and evaluate a *ResNet-18* architecture specific to CIFAR10 (details about the ImageNet model can be found here: https://arxiv.org/abs/1512.03385). Please report the accuracy obtained on the whole dataset as well as the reference paper/GitHub link.\n",
        "\n",
        "*Hint :* You can re-use the following code : https://github.com/kuangliu/pytorch-cifar. During a training of 10 epochs, a batch size of 10 and a learning rate of 0.01, one obtains 40% accuracy on $X_{\\text{train}}$ (\\~2 minutes) and 20% accuracy on $X_{\\text{test}}$ (\\~5 minutes)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For reproductibility\n",
        "import torch\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6CEbg6HXp75",
        "outputId": "2bf4147a-93e1-499b-e43b-ee861fc4b30f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fdb7e0f45d0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model from https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])"
      ],
      "metadata": {
        "id": "0biFiolclyDg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RVHhKmWN_2hj"
      },
      "outputs": [],
      "source": [
        "# Defining training loop\n",
        "def train(model, loss_fcn, score_fcn, device, optimizer, max_epochs, train_dataloader, val_dataloader=None):\n",
        "\n",
        "    epoch_list = []\n",
        "    scores_list = []\n",
        "\n",
        "    # loop over epochs\n",
        "    for epoch in range(max_epochs):\n",
        "        model.train()\n",
        "        batch_correct = []\n",
        "        losses = []\n",
        "        # loop over batches\n",
        "        for i, train_batch in enumerate(train_dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            x, y = train_batch\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            # logits is the output of the model\n",
        "            logits = model(x)\n",
        "            # compute the loss\n",
        "            loss = loss_fcn(logits, y)\n",
        "            # optimizer step\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses.append(loss.item())\n",
        "            score = score_fcn(logits.cpu(), y.cpu())\n",
        "            batch_correct.append(score * x.shape[0])\n",
        "\n",
        "        loss_data = np.array(losses).mean()\n",
        "        score_epoch = np.sum(batch_correct)/len(train_loader.dataset)\n",
        "        print(\"Epoch {:05d} | Loss: {:.4f} | Accuracy: {:.4f}\".format(epoch + 1, loss_data, score_epoch))\n",
        "\n",
        "        if epoch % 5 == 0 and val_dataloader is not None:\n",
        "            # evaluate the model on the validation set\n",
        "            # computes the f1-score (see next function)\n",
        "            score = evaluate(model, loss_fcn, device, val_dataloader)\n",
        "            print(\"Accuracy: {:.4f}\".format(score))\n",
        "            scores_list.append(score)\n",
        "            epoch_list.append(epoch)\n",
        "\n",
        "    return epoch_list, scores_list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loss_fcn, score_fcn, device, dataloader):\n",
        "    score_list_batch = []\n",
        "\n",
        "    model.eval()\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        x, y = batch\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        output = model(x)\n",
        "        loss_test = loss_fcn(output, y)\n",
        "        #predict = np.where(output.detach().cpu().numpy() >= 0, 1, 0)\n",
        "        score = score_fcn(output.cpu(), y.cpu())\n",
        "        score_list_batch.append(score)\n",
        "\n",
        "    return np.array(score_list_batch).mean()"
      ],
      "metadata": {
        "id": "z9-CxshAnB5Q"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "learning_rate = 0.001\n",
        "max_epoch = 10\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_100, batch_size=batch_size,\n",
        "                                           shuffle=True, num_workers=2)\n",
        "model = ResNet18()\n",
        "loss_fcn = F.cross_entropy\n",
        "score_fcn = Accuracy(task='multiclass', num_classes=10)\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "_, _ = train(model, loss_fcn, score_fcn, device, optimizer, max_epoch, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkZ-3EYhnmSY",
        "outputId": "b35e079f-ca0f-4b61-f9f0-e6429da62701"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 00001 | Loss: 2.9649 | Accuracy: 0.1100\n",
            "Epoch 00002 | Loss: 2.1273 | Accuracy: 0.2500\n",
            "Epoch 00003 | Loss: 1.8026 | Accuracy: 0.3900\n",
            "Epoch 00004 | Loss: 1.6485 | Accuracy: 0.4100\n",
            "Epoch 00005 | Loss: 1.5220 | Accuracy: 0.4700\n",
            "Epoch 00006 | Loss: 1.4761 | Accuracy: 0.5100\n",
            "Epoch 00007 | Loss: 1.3651 | Accuracy: 0.5000\n",
            "Epoch 00008 | Loss: 1.0783 | Accuracy: 0.6300\n",
            "Epoch 00009 | Loss: 0.9557 | Accuracy: 0.6100\n",
            "Epoch 00010 | Loss: 0.9365 | Accuracy: 0.6800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_set, \n",
        "                                          batch_size=batch_size, \n",
        "                                          num_workers=2)\n",
        "\n",
        "full_loader = torch.utils.data.DataLoader(full_set, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "acc_test_quest_3=evaluate(model, loss_fcn, score_fcn, device, test_loader)\n",
        "acc_whole_quest_3=evaluate(model, loss_fcn, score_fcn, device, full_loader)\n",
        "print('Evaluation on test dataset:',acc_test_quest_3)\n",
        "print('Evaluation on whole dataset:',acc_whole_quest_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nuuLT8B208L",
        "outputId": "34498df8-547a-4176-ac4d-553441fc8249"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on test dataset: 0.2361\n",
            "Evaluation on whole dataset: 0.23505001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Model | Number of  epochs  | Train accuracy | Test accuracy | Whole dataset accuracy | Reference paper/Github link |\n",
        "|------|------|------|------| -----| -----|\n",
        "|   ResNet-18  | 10 | 0.6700 | 0.2326 | 0.23245001 | https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py |\n",
        "\n",
        "We can see that the baseline model overfits on the train dataset, achieving quite high accuracy. However, as a result of overfitting the accuracy is quite low in the test dataset."
      ],
      "metadata": {
        "id": "oLibZPu33mg1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C3mHqCk_2hj"
      },
      "source": [
        "# Transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Tn9pW14_2hj"
      },
      "source": [
        "We propose to use pre-trained models on a classification and generative task, in order to improve the results of our setting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8g_3ZDi_2hj"
      },
      "source": [
        "## ImageNet features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfYEhdFb_2hj"
      },
      "source": [
        "Now, we will use some pre-trained models on ImageNet and see how well they compare on CIFAR. A list is available on : https://pytorch.org/vision/stable/models.html.\n",
        "\n",
        "__Question 4 (3 points):__ Pick a model from the list above, adapt it for CIFAR10 and retrain its final layer (or a block of layers, depending on the resources to which you have access to). Report its accuracy."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "n_samples = 100\n",
        "batch_size = 10\n",
        "\n",
        "train_set = datasets.CIFAR10(root='./data', train=True,\n",
        "                             download=True, transform=train_transform)\n",
        "train_100 = torch.utils.data.Subset(train_set, np.arange(n_samples)) \n",
        "\n",
        "test_set = datasets.CIFAR10(root='./data', train=False,\n",
        "                             download=True, transform=test_transform)\n",
        "\n",
        "full_set = torch.utils.data.ConcatDataset([train_set, test_set])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KASANtGmp0sP",
        "outputId": "0918d2a6-ebf6-4c06-9513-e88791578177"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "id": "i_lh4xje_2hk"
      },
      "outputs": [],
      "source": [
        "# From Pytorch\n",
        "class ResNetCIFAR(nn.Module):\n",
        "    def __init__(self,\n",
        "                 use_pretrained:bool = True,\n",
        "                 retrain_all:bool = True):\n",
        "        super(ResNetCIFAR, self).__init__()\n",
        "        if use_pretrained:\n",
        "            base_model = models.resnet50(weights = \"IMAGENET1K_V2\")\n",
        "        else:\n",
        "            base_model = models.resnet50(weights = None)\n",
        "        num_ftrs = base_model.fc.in_features\n",
        "        self.base_model = torch.nn.Sequential(*(list(base_model.children())[:-1]))\n",
        "        self.classifier = nn.Linear(num_ftrs, 10)\n",
        "\n",
        "        for param in self.base_model.parameters():\n",
        "            param.requires_grad = retrain_all\n",
        "\n",
        "    def forward(self, x:torch.Tensor):\n",
        "        x = self.base_model(x)\n",
        "        x = self.classifier(x.squeeze())\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "learning_rate = 0.001\n",
        "max_epoch = 20\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_100, batch_size=batch_size,\n",
        "                                           shuffle=True, num_workers=2)\n",
        "model = ResNetCIFAR(use_pretrained=True, retrain_all=True)\n",
        "loss_fcn = F.cross_entropy\n",
        "score_fcn = Accuracy(task='multiclass', num_classes=10)\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\n",
        "#optimizer = torch.optim.Adam([{'params': model.base_model.parameters()},\n",
        "#                              {'params': model.classifier.parameters(), 'lr': 1e-4}], lr=1e-3)\n",
        "model = model.to(device)\n",
        "\n",
        "_, _ = train(model, loss_fcn, score_fcn, device, optimizer, max_epoch, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424,
          "referenced_widgets": [
            "3d3abeec7ed84d568a80a63e810cb0a6",
            "6a37591839d44dc49404b7ba22706921",
            "9254339fb99f493cb2ba443a274a7038",
            "ab7cd16c0f924cd288f539759a9a5045",
            "1cd21030653444ab9436a1d9df78c9d9",
            "d2c34beafad54422a364bb644e14bfb2",
            "2e64f0e25d104fe98fcdcd52a0041e4c",
            "3bc477cd09454435a1af8fa3244bdf00",
            "4be23206b71a482c9c0a013566612674",
            "4177b7e34446470ab75ef31408dd3649",
            "8e20b5ca23a4442ea68b8e0205403035"
          ]
        },
        "id": "rYlOdqqz69_j",
        "outputId": "40561552-4b3c-4766-89aa-b89e1e758293"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d3abeec7ed84d568a80a63e810cb0a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 00001 | Loss: 2.3767 | Accuracy: 0.0900\n",
            "Epoch 00002 | Loss: 2.2225 | Accuracy: 0.2500\n",
            "Epoch 00003 | Loss: 2.1146 | Accuracy: 0.2700\n",
            "Epoch 00004 | Loss: 2.0532 | Accuracy: 0.3000\n",
            "Epoch 00005 | Loss: 1.8595 | Accuracy: 0.4300\n",
            "Epoch 00006 | Loss: 1.6836 | Accuracy: 0.5200\n",
            "Epoch 00007 | Loss: 1.3652 | Accuracy: 0.6300\n",
            "Epoch 00008 | Loss: 1.3950 | Accuracy: 0.6400\n",
            "Epoch 00009 | Loss: 1.3046 | Accuracy: 0.5900\n",
            "Epoch 00010 | Loss: 0.9505 | Accuracy: 0.7700\n",
            "Epoch 00011 | Loss: 0.7705 | Accuracy: 0.8000\n",
            "Epoch 00012 | Loss: 0.6389 | Accuracy: 0.8200\n",
            "Epoch 00013 | Loss: 0.6782 | Accuracy: 0.7700\n",
            "Epoch 00014 | Loss: 0.5190 | Accuracy: 0.8700\n",
            "Epoch 00015 | Loss: 0.3793 | Accuracy: 0.9300\n",
            "Epoch 00016 | Loss: 0.3640 | Accuracy: 0.9000\n",
            "Epoch 00017 | Loss: 0.3849 | Accuracy: 0.8900\n",
            "Epoch 00018 | Loss: 0.5032 | Accuracy: 0.8600\n",
            "Epoch 00019 | Loss: 0.4728 | Accuracy: 0.8800\n",
            "Epoch 00020 | Loss: 0.3762 | Accuracy: 0.9100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_set, \n",
        "                                          batch_size=batch_size, \n",
        "                                          num_workers=2)\n",
        "\n",
        "full_loader = torch.utils.data.DataLoader(full_set, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "acc_test_quest_4=evaluate(model, loss_fcn, score_fcn, device, test_loader)\n",
        "acc_whole_quest_4=evaluate(model, loss_fcn, score_fcn, device, full_loader)\n",
        "print('Evaluation on test dataset:',acc_test_quest_4)\n",
        "print('Evaluation on whole dataset:',acc_whole_quest_4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgB3Irq58BC8",
        "outputId": "66c9decf-90e7-4308-d6dc-5df47ebe6bc2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on test dataset: 0.34090003\n",
            "Evaluation on whole dataset: 0.34318334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Model | Number of  epochs  | Train accuracy | Test accuracy | Whole dataset accuracy | Reference |\n",
        "|------|------|------|------| -----| ------|\n",
        "|   ResNetCIFAR  | 20 | 0.9100 | 0.34090003 | 0.34318334 | https://arxiv.org/abs/1512.03385\n",
        "\n",
        "For this model, we obtain higher accuracy for both the train and test datasets, indicating the positive impact of the transfer learning use. However, we keep overfitting the train dataset, as it can be seen in the accuracy results."
      ],
      "metadata": {
        "id": "b79feKJn4foT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvkuMzLs_2hk"
      },
      "source": [
        "# Incorporating *a priori*\n",
        "Geometrical *a priori* are appealing for image classification tasks. For now, we only consider linear transformations $T$ of the inputs $x:\\mathbb{S}^2\\rightarrow\\mathbb{R}$ where $\\mathbb{S}$ is the support of an image, meaning that :\n",
        "\n",
        "$$\\forall u\\in\\mathbb{S}^2,T(\\lambda x+\\mu y)(u)=\\lambda T(x)(u)+\\mu T(y)(u)\\,.$$\n",
        "\n",
        "For instance if an image had an infinite support, a translation $T_a$ by $a$ would lead to :\n",
        "\n",
        "$$\\forall u, T_a(x)(u)=x(u-a)\\,.$$\n",
        "\n",
        "Otherwise, one has to handle several boundary effects.\n",
        "\n",
        "__Question 5 (1.5 points) :__ Explain the issues when dealing with translations, rotations, scaling effects, color changes on $32\\times32$ images. Propose several ideas to tackle them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIaY60o1_2hk"
      },
      "source": [
        "The main problems on applying these types of transformations on $32 \\times 32 $ images are related to the small size of images and the fact that the considered models can easily miss-classify images. Generally, any of these transformations can lead to a too large distortion of the image making them useless to train the model. In addition, some transformations might reduce the number of possible pixels to use, in the case of such small size, the number of lost pixels might be too big to consider the image useful for training.\n",
        "\n",
        "We consider two main ways to tackle them:\n",
        "\n",
        "- to resize the images to make them bigger: hence, we can apply the transformation to the resized image.\n",
        "- to make only small transformations: for example, if we want to perform a rotation, only considering a small degree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds6e6teG_2hk"
      },
      "source": [
        "## Data augmentations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ek5wlOo_2hk"
      },
      "source": [
        "__Question 6 (3 points):__ Propose a set of geometric transformation beyond translation, and incorporate them in your training pipeline. Train the model of the __Question 3__ with them and report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(224*112, num_classes) #modifying the architecture \n",
        "        #to allow for bigger images\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])"
      ],
      "metadata": {
        "id": "-j-WWYPvdp4m"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": true,
        "id": "FqCjrXGk_2hk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8feb1fea-1cbd-460e-b6cc-7d1a4a715f27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.RandomChoice([\n",
        "        transforms.RandomHorizontalFlip(0.6),\n",
        "        transforms.RandomVerticalFlip(0.6),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.03, saturation=0.03, hue=0.03),\n",
        "        transforms.RandomAffine(degrees=(0, 15), translate=(0.0, 0.1),),\n",
        "    ]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "n_samples = 100\n",
        "batch_size = 10\n",
        "\n",
        "train_set = datasets.CIFAR10(root='./data', train=True,\n",
        "                             download=True, transform=train_transform)\n",
        "train_100 = torch.utils.data.Subset(train_set, np.arange(n_samples)) \n",
        "\n",
        "test_set = datasets.CIFAR10(root='./data', train=False,\n",
        "                            download=True, transform=test_transform)\n",
        "\n",
        "full_set = torch.utils.data.ConcatDataset([train_set, test_set])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "learning_rate = 1e-4\n",
        "max_epoch = 20\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_100, batch_size=batch_size,\n",
        "                                           shuffle=True, num_workers=2)\n",
        "\n",
        "model = ResNet18()\n",
        "loss_fcn = F.cross_entropy\n",
        "score_fcn = Accuracy(task='multiclass', num_classes=10)\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\n",
        "model = model.to(device)\n",
        "\n",
        "_, _ = train(model, loss_fcn, score_fcn, device, optimizer, max_epoch, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6k9F9gZfNXq",
        "outputId": "eb3b8851-4e55-402e-bd39-c982f5ce58ed"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 00001 | Loss: 3.0731 | Accuracy: 0.1900\n",
            "Epoch 00002 | Loss: 2.0617 | Accuracy: 0.3700\n",
            "Epoch 00003 | Loss: 1.5645 | Accuracy: 0.5200\n",
            "Epoch 00004 | Loss: 1.3821 | Accuracy: 0.5200\n",
            "Epoch 00005 | Loss: 0.9307 | Accuracy: 0.7200\n",
            "Epoch 00006 | Loss: 1.0040 | Accuracy: 0.7100\n",
            "Epoch 00007 | Loss: 0.8509 | Accuracy: 0.7600\n",
            "Epoch 00008 | Loss: 0.6121 | Accuracy: 0.8000\n",
            "Epoch 00009 | Loss: 0.4826 | Accuracy: 0.8000\n",
            "Epoch 00010 | Loss: 0.5423 | Accuracy: 0.8600\n",
            "Epoch 00011 | Loss: 0.7540 | Accuracy: 0.7700\n",
            "Epoch 00012 | Loss: 0.6566 | Accuracy: 0.8000\n",
            "Epoch 00013 | Loss: 0.3840 | Accuracy: 0.9000\n",
            "Epoch 00014 | Loss: 0.4506 | Accuracy: 0.8700\n",
            "Epoch 00015 | Loss: 0.4993 | Accuracy: 0.8500\n",
            "Epoch 00016 | Loss: 0.4184 | Accuracy: 0.8600\n",
            "Epoch 00017 | Loss: 0.3266 | Accuracy: 0.8800\n",
            "Epoch 00018 | Loss: 0.3347 | Accuracy: 0.8700\n",
            "Epoch 00019 | Loss: 0.3482 | Accuracy: 0.8900\n",
            "Epoch 00020 | Loss: 0.3817 | Accuracy: 0.8700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_set, \n",
        "                                          batch_size=batch_size, \n",
        "                                          num_workers=2)\n",
        "\n",
        "full_loader = torch.utils.data.DataLoader(full_set, batch_size=batch_size)\n",
        "\n",
        "acc_test_quest_6=evaluate(model, loss_fcn, score_fcn, device, test_loader)\n",
        "print('Evaluation on test dataset:',acc_test_quest_6)\n",
        "acc_whole_quest_6=evaluate(model, loss_fcn, score_fcn, device, full_loader)\n",
        "print('Evaluation on whole dataset:',acc_whole_quest_6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mo7z2joDhL1m",
        "outputId": "8a02efe0-6940-4443-f80b-250625fc4abb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on test dataset: 0.2407\n",
            "Evaluation on whole dataset: 0.23605001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Model | Number of  epochs  | Train accuracy | Test accuracy | Whole dataset accuracy | GitHub |\n",
        "|------|------|------|------| -----| ----|\n",
        "|   ResNetCIFAR  | 20 | 0.8700 | 0.2407 | 0.2360500 | https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py\n",
        "\n",
        "For this model, we obtain higher accuracy for both the train and test datasets than the model from **Question 3**. However, we keep overfitting the train dataset, as it can be seen in the accuracy results. Using a high variety of transformations in the data augmentation step can lead to better results. This would nevertheless requires us to train for longer. Since we are working under the assumption of a low computational budget we decided not to go down this route.\n",
        "\n",
        "We see then, that in the case of training with few samples, transfer learning seems to be the better solution. It will be even better to mix transfer learning with data augmentation."
      ],
      "metadata": {
        "id": "odA0ymac5sTu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRUA5I8N_2hk"
      },
      "source": [
        "# Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmyiWAPJ_2hl"
      },
      "source": [
        "__Question 7 (5 points) :__ Write a short report explaining the pros and the cons of each method that you implemented. 25% of the grade of this project will correspond to this question, thus, it should be done carefully. In particular, please add a plot that will summarize all your numerical results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ-v4Nev_2hl"
      },
      "source": [
        "\n",
        "Our aim was to build a good model for the CIFAR10 dataset where we have a train set with only 100 labeled pictures and with a limitated computational budget. For this, we considered as baseline model the ResNet18. Afterwards, we improve the results by performing transfer learning (using a pre-trained model) and data augmentation.\n",
        "\n",
        "**Baseline: ResNet18**\n",
        "\n",
        "The baseline model was the ResNet18, a convolutional network with residual connection. In this case, we trained it with the 100 labels from scratch and we obtained a very small performance. However, the computational cost was the smallest and it does not depend on the existence of pre-trained models.\n",
        "\n",
        "**First improvement try: Transfer Learning**\n",
        "\n",
        "In this case, we consider the architecture pre-trained on ImageNet and we retrained the last fully connected layer with our labeled train dataset from CIFAR10 dataset. In this case, we obtained the best accuracy results for the test dataset and the whole dataset. However, it required to transform the resize the dataset into the same shape of the one used for the pre-training. In addition, it requires the existence of the pre-trained model, which might not be always the case.\n",
        "\n",
        "**Second improvement try: Data Augmentation**\n",
        "\n",
        "Firstly, we consider a resize to avoid a negative effect of the data augmentation, which probably happen on small size images. Then, we applied different transformations in order to try to extract more information from the train dataset. In this case, we consider a small increasement on the performance in relation with the baseline. We highlight that the increasement could be bigger if we consider different transformations of the augmentation. However, if the impact of these transformations is too big it leads to very bad performance results due to bad-quality training images and a bad labelling, for more information https://arxiv.org/abs/1507.06535 or https://openreview.net/pdf?id=BJfvknCqFQ can be seen. Moreover, this technique requires the higher computational cost.\n",
        "\n",
        "**Discussion and possible follow-ups**\n",
        "We highlight that the performance for the three models are quite similar on the test dataset and the whole dataset, which can be explained taking into account that almost the whole dataset can be considered as test dataset since there were only 100 observations used for training. Moreover, there is a huge overfitting on the three considered cases, as it can be seen in the figure below. We want to highlight that the results show that, if it exists, using a pre-trained model can be the best solution. Also, the improvement of the performance on the augmentation method might not justify it computational cost and the fact that it can have a negative impact if the transformations are not well-chosen. To conclude, we want to mention that there are other possibilities to consider like combining a pre-trained model with augmentation or, in this case where there are no-labeled train observations, to consider a weakly supervised method."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "n_jBfoC5su5X"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracies = [0.6700, 0.9000, 0.8900]\n",
        "test_accuracies = [acc_test_quest_3,acc_test_quest_4,acc_test_quest_6]\n",
        "full_accuracies = [acc_whole_quest_3,acc_whole_quest_4,acc_whole_quest_6]\n",
        "\n",
        "train_bars = [i for i in range(len(train_accuracies))]\n",
        "test_bars = [x + 0.2 for x in train_bars]\n",
        "whole_bars= [x + 0.4 for x in train_bars]\n",
        "\n",
        "plt.bar(train_bars, train_accuracies, color ='b', width = 0.2, edgecolor ='grey', label ='train')\n",
        "plt.bar(test_bars, test_accuracies, color ='g', width = 0.2, edgecolor ='grey', label ='test')\n",
        "plt.bar(whole_bars, test_accuracies, color ='r', width = 0.2, edgecolor ='grey', label ='whole')\n",
        "\n",
        "plt.xticks([i+0.2 for i in range(len(train_accuracies))],\n",
        "        [\"Baseline\", \"Pretrained\", \"Augmentation\"])\n",
        "\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy for the different methods\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JIn3JVjbQn-T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "72cac6a1-066d-4ede-9065-88c5693a1162"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAGp7ddN_2hl"
      },
      "source": [
        "# Weak supervision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHQRLbC3_2hl"
      },
      "source": [
        "__Bonus \\[open\\] question (up to 3 points) :__ Pick a weakly supervised method that will potentially use $X\\cup X_{\\text{train}}$ to train a representation (a subset of $X$ is also fine). Evaluate it and report the accuracies. You should be careful in the choice of your method, in order to avoid heavy computational effort."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a weakly supervised method without heavy computational effort we propose label-propagation to use in $X \\cup X_{\\text{train}}$. This semi-supervised technique propagate labels to the unlabeled data. Then, we can use these new labels to train our ResNet18."
      ],
      "metadata": {
        "id": "8sRaxcxbwvWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.semi_supervised import LabelPropagation"
      ],
      "metadata": {
        "id": "0Rd18mQixBdg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = datasets.CIFAR10(root='./data', train=True,\n",
        "                             download=True, transform=train_transform)\n",
        "train_100 = torch.utils.data.Subset(train_set, np.arange(n_samples))\n",
        "\n",
        "test_set = datasets.CIFAR10(root='./data', train=False,\n",
        "                             download=True, transform=test_transform)\n",
        "\n",
        "full_set = torch.utils.data.ConcatDataset([train_set, test_set])\n",
        "train_unlab = torch.utils.data.Subset(train_set, np.arange(100, len(train_set)))"
      ],
      "metadata": {
        "id": "eCLOMGjk1qNy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e6b10f3-1cd9-4f7f-90e3-b01d28db2953"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labeled_features = []\n",
        "train_labeled_labels = []\n",
        "for i in np.arange(100):\n",
        "    train_labeled_features.append(train_set[i][0].numpy().flatten())\n",
        "    train_labeled_labels.append(train_set[i][1])\n",
        "train_labeled_labels = np.reshape(train_labeled_labels, (-1, 1))\n",
        "\n",
        "label_prop_model = LabelPropagation(kernel='knn', n_neighbors=5)\n",
        "label_prop_model.fit(train_labeled_features, train_labeled_labels)\n",
        "\n",
        "train_unlab_features = []\n",
        "for i in np.arange(100, len(train_set)):\n",
        "    train_unlab_features.append(train_set[i][0].numpy().flatten())\n",
        "train_unlab_labels = label_prop_model.predict(train_unlab_features)\n",
        "\n",
        "train_unlab_labels = train_unlab_labels.reshape(len(train_unlab_labels), 1)\n",
        "train_labels = np.concatenate((train_labeled_labels, train_unlab_labels))\n",
        "train_features = train_labeled_features + train_unlab_features"
      ],
      "metadata": {
        "id": "M4H2fEZy1rzy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31e7d9af-9d3c-488e-ce37-dc85943fccb4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = list(zip(train_features, train_labels))\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "RIKJ_RDc64HD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "learning_rate = 0.001\n",
        "max_epoch = 20\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_100, batch_size=batch_size,\n",
        "                                           shuffle=True, num_workers=2)\n",
        "model = ResNet18()\n",
        "loss_fcn = F.cross_entropy\n",
        "score_fcn = Accuracy(task='multiclass', num_classes=10)\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "_, _ = train(model, loss_fcn, score_fcn, device, optimizer, max_epoch, train_loader)"
      ],
      "metadata": {
        "id": "MzxJOnZ5wur0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f11c01c-2a34-4879-a261-9f6ab507534b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 00001 | Loss: 3.1831 | Accuracy: 0.0900\n",
            "Epoch 00002 | Loss: 2.4273 | Accuracy: 0.1700\n",
            "Epoch 00003 | Loss: 2.0323 | Accuracy: 0.2900\n",
            "Epoch 00004 | Loss: 1.8549 | Accuracy: 0.3900\n",
            "Epoch 00005 | Loss: 1.7824 | Accuracy: 0.3600\n",
            "Epoch 00006 | Loss: 1.6809 | Accuracy: 0.4400\n",
            "Epoch 00007 | Loss: 1.5765 | Accuracy: 0.4500\n",
            "Epoch 00008 | Loss: 1.4603 | Accuracy: 0.4400\n",
            "Epoch 00009 | Loss: 1.4127 | Accuracy: 0.5100\n",
            "Epoch 00010 | Loss: 1.2424 | Accuracy: 0.5800\n",
            "Epoch 00011 | Loss: 1.2696 | Accuracy: 0.5500\n",
            "Epoch 00012 | Loss: 1.1298 | Accuracy: 0.6700\n",
            "Epoch 00013 | Loss: 1.0438 | Accuracy: 0.6000\n",
            "Epoch 00014 | Loss: 0.8461 | Accuracy: 0.7000\n",
            "Epoch 00015 | Loss: 0.7882 | Accuracy: 0.7200\n",
            "Epoch 00016 | Loss: 0.6225 | Accuracy: 0.8200\n",
            "Epoch 00017 | Loss: 0.6400 | Accuracy: 0.8200\n",
            "Epoch 00018 | Loss: 0.5869 | Accuracy: 0.8300\n",
            "Epoch 00019 | Loss: 0.6330 | Accuracy: 0.8300\n",
            "Epoch 00020 | Loss: 0.5181 | Accuracy: 0.7900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_set, \n",
        "                                          batch_size=batch_size, \n",
        "                                          num_workers=2)\n",
        "full_loader = torch.utils.data.DataLoader(full_set, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print('Evaluation on test dataset:',evaluate(model, loss_fcn, score_fcn, device, test_loader))\n",
        "print('Evaluation on whole dataset:',evaluate(model, loss_fcn, score_fcn, device, full_loader))"
      ],
      "metadata": {
        "id": "wl8mXpEl7q0g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9f01ddd-1030-46bd-e72f-bf69d27cc8b8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on test dataset: 0.2505\n",
            "Evaluation on whole dataset: 0.24745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Model | Number of  epochs  | Train accuracy | Test accuracy | Whole dataset accuracy\t\n",
        "|------|------|------|------|---|\n",
        "|   LabelPropagation+ResNet18  | 20 | 0.7900 | 0.2505 | 0.24745|"
      ],
      "metadata": {
        "id": "pYDlrLYk73Jp"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3d3abeec7ed84d568a80a63e810cb0a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a37591839d44dc49404b7ba22706921",
              "IPY_MODEL_9254339fb99f493cb2ba443a274a7038",
              "IPY_MODEL_ab7cd16c0f924cd288f539759a9a5045"
            ],
            "layout": "IPY_MODEL_1cd21030653444ab9436a1d9df78c9d9"
          }
        },
        "6a37591839d44dc49404b7ba22706921": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2c34beafad54422a364bb644e14bfb2",
            "placeholder": "​",
            "style": "IPY_MODEL_2e64f0e25d104fe98fcdcd52a0041e4c",
            "value": "100%"
          }
        },
        "9254339fb99f493cb2ba443a274a7038": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bc477cd09454435a1af8fa3244bdf00",
            "max": 102540417,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4be23206b71a482c9c0a013566612674",
            "value": 102540417
          }
        },
        "ab7cd16c0f924cd288f539759a9a5045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4177b7e34446470ab75ef31408dd3649",
            "placeholder": "​",
            "style": "IPY_MODEL_8e20b5ca23a4442ea68b8e0205403035",
            "value": " 97.8M/97.8M [00:01&lt;00:00, 56.7MB/s]"
          }
        },
        "1cd21030653444ab9436a1d9df78c9d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2c34beafad54422a364bb644e14bfb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e64f0e25d104fe98fcdcd52a0041e4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3bc477cd09454435a1af8fa3244bdf00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4be23206b71a482c9c0a013566612674": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4177b7e34446470ab75ef31408dd3649": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e20b5ca23a4442ea68b8e0205403035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}